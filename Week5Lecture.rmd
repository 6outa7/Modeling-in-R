```{r}
options(tidyverse.quiet = TRUE)
library(titanic)
library(tidyverse)
library(mice) #package for imputation
library(VIM) #visualizing missingness
library(ranger) #ranger package for random forests
library(caret) #control model building
library(e1071) #often needed for model building assistance
```

Read in dataset     
```{r}
titanic = titanic::titanic_train
```

Structure and summary  
```{r}
str(titanic)
summary(titanic)
```

Factor conversion and recoding  
```{r}
titanic = titanic %>% mutate(Survived = as.factor(Survived)) %>% 
  mutate(Survived = fct_recode(Survived, "No" = "0", "Yes" = "1" )) %>%
  mutate(Pclass = as.factor(Pclass)) %>% mutate(Sex = as.factor(Sex)) %>%
  mutate(Embarked = as.factor(Embarked)) %>% 
  mutate(Embarked = fct_recode(Embarked,"Unknown"="","Cherbourg"="C","Southampton"="S","Queenstown"="Q"))

str(titanic)
```

Column-wise deletion of the "Cabin" variable.  
```{r}
titanic = titanic %>% select(-Cabin) 
```

```{r}
#select only variables relevant to our analysis
titanic = titanic %>% select(c("Survived","Pclass","Sex","Age","SibSp","Parch","Embarked"))

imp_age = mice(titanic, m=1, method='pmm', printFlag=FALSE)
summary(imp_age)
```

Merge the imputed values into our titanic data frame  
```{r}
titanic_complete = complete(imp_age) 
summary(titanic_complete)
```

Split the data
```{r}
set.seed(123)
train.rows = createDataPartition(y = titanic_complete$Survived,p=0.7,list=FALSE)
train = slice(titanic_complete, train.rows)
test = slice(titanic_complete, -train.rows)
```

Now that we have the imputed/complete data, let's build a random forest model and tune the "mtry" parameter by grid search.  
```{r}
# fit_control = trainControl(method = "cv",  
#                            number = 10) #set up 10 fold cross-validation
# 
# tunegrid = expand.grid(mtry = 1:6, splitrule = "gini", min.node.size=1) 
# #mtry: Typically square root of number of predictors (we have 6 predictors in this model, so would be 2 or 3), however, we have such a small number of predictors we can test mtry values from 1 to 6 and it won't be too computationally expensive
# 
# #splitrule: The ranger package provides three options for the splitrule parameter: gini, extratrees, and hellinger. gini is the default split rule and determines splits to improve purity. extratrees splits at random. hellinger is a split rule designed for imbalanced data (data where one of the classes of the binary response variable has many more observations than the other)
# 
# #min.node.size: If a split is made, how many observations must be in a node. Default is 1.    
# 
# set.seed(123)  
# rf_fit = train(x=train[,-1],y=train$Survived, 
#                #be careful to select the correct variable to exclude from the predictors (x's)
#                method = "ranger",  
#                tuneGrid = tunegrid, #instruct model to use tuning grid
#                importance = "permutation", 
#                #I like to be able to see the variable importance results, with ranger you indicate that you want to see them
#                trControl = fit_control)
```

View results  
```{r}
print(rf_fit)
plot(rf_fit)
varImp(rf_fit)
```

Predictions on the training set 
```{r}
predRF = predict(rf_fit, train, type = "raw")
```

Confusion matrix for training set
```{r}
confusionMatrix(predRF, train$Survived, positive = "Yes")
```

Predictions on the test set
```{r}
predRFtest = predict(rf_fit, newdata = test, type = "raw")
```

Confusion matrix for test set
```{r}
confusionMatrix(predRFtest, test$Survived, positive = "Yes")
```

Let's look at a larger tuning grid
```{r}
#The bigger the grid, the more models that need to be built and the longer your model runs will take. 
#If you have extremely long run times consider the following (you can do some or all of these):
#1. Reduce the size of the tuning grid (tune on few parameters and/or fewer values of the parameters)
#2. Reduce the number of folds in the k-fold cross-validation. Reducing from 10 to 5 or 3 can make a huge difference in computation time
#3. Reduce the size of the training set (do a 60/40 split if you have lots of data and are running into run time issues)
#4. Sample. If you have a huge dataset, consider sampling from the dataset, then doing your train/test split on the sample

# tunegrid <- expand.grid(mtry = 1:6, splitrule = c("gini","extratrees","hellinger"), min.node.size=1:5) 
# #mtry: Typically square root of number of predictors (we have 6 predictors in this model, so would be 2 or 3), however, we have such a small number of predictors we can test mtry values from 1 to 6 and it won't be too computationally expensive
# 
# #splitrule: The ranger package provides three options for the splitrule parameter: gini, extratrees, and hellinger. gini is the default split rule and determines splits to improve purity. extratrees splits at random. hellinger is a split rule designed for imbalanced data (data where one of the classes of the binary response variable has many more observations than the other)
# 
# #min.node.size: If a split is made, how many observations must be in a node. Default is 1.    
# 
# set.seed(123)  
# rf_fit2 = train(x=train[,-1],y=train$Survived, 
#                #be careful to select the correct variable to exclude from the predictors (x's)
#                method = "ranger",  
#                tuneGrid = tunegrid, #instruct model to use tuning grid
#                importance = "permutation", 
#                #I like to be able to see the variable importance results, with ranger you indicate that you want to see them
#                trControl = fit_control)
```

```{r}
print(rf_fit2)
plot(rf_fit2)
varImp(rf_fit2)
```

Predictions on the training set 
```{r}
predRF2 = predict(rf_fit2, train, type = "raw")
```

Confusion matrix for training set
```{r}
confusionMatrix(predRF2, train$Survived, positive = "Yes")
```

Predictions on the test set
```{r}
predRFtest2 = predict(rf_fit2, newdata = test, type = "raw")
```

Confusion matrix for test set
```{r}
confusionMatrix(predRFtest2, test$Survived, positive = "Yes")
```

One more thing to try. Adding a max.depth parameter.  
```{r}
#The bigger the grid, the more models that need to be built and the longer your model runs will take. 
#If you have extremely long run times consider the following (you can do some or all of these):
#1. Reduce the size of the tuning grid (tune on few parameters and/or fewer values of the parameters)
#2. Reduce the number of folds in the k-fold cross-validation. Reducing from 10 to 5 or 3 can make a huge difference in computation time
#3. Reduce the size of the training set (do a 60/40 split if you have lots of data and are running into run time issues)
#4. Sample. If you have a huge dataset, consider sampling from the dataset, then doing your train/test split on the sample
# 
# tunegrid <- expand.grid(mtry = 1, splitrule = c("gini"), min.node.size=3) ###SELECTED OPTIMAL parameters from tuning
# #mtry: Typically square root of number of predictors (we have 6 predictors in this model, so would be 2 or 3), however, we have such a small number of predictors we can test mtry values from 1 to 6 and it won't be too computationally expensive
# 
# #splitrule: The ranger package provides three options for the splitrule parameter: gini, extratrees, and hellinger. gini is the default split rule and determines splits to improve purity. extratrees splits at random. hellinger is a split rule designed for imbalanced data (data where one of the classes of the binary response variable has many more observations than the other)
# 
# #min.node.size: If a split is made, how many observations must be in a node. Default is 1.
# 
# set.seed(123)
# rf_fit3 = train(x=train[,-1],y=train$Survived,
#                #be careful to select the correct variable to exclude from the predictors (x's)
#                method = "ranger",
#                tuneGrid = tunegrid, #instruct model to use tuning grid
#                importance = "permutation",
#                max.depth = 5, ###NEW LINE here
#                #I like to be able to see the variable importance results, with ranger you indicate that you want to see them
#                trControl = fit_control)
```
```{r}
rf_fit3 = readRDS("rf_fit3.rds")
```

```{r}
print(rf_fit3)
#plot(rf_fit3)
varImp(rf_fit3)
```

Predictions on the training set 
```{r}
predRF3 = predict(rf_fit3, train, type = "raw")
```

Confusion matrix for training set
```{r}
confusionMatrix(predRF3, train$Survived, positive = "Yes")
```

Predictions on the test set
```{r}
predRFtest3 = predict(rf_fit3, newdata = test, type = "raw")
```

Confusion matrix for test set
```{r}
confusionMatrix(predRFtest3, test$Survived, positive = "Yes")
```

Summarize our model results (can compare results for any caret models)   
```{r}
results = resamples(list(RFmtry=rf_fit, RFtune=rf_fit2, RFMaxDepth=rf_fit3))
# boxplots of results
bwplot(results)
# dotplots of results
dotplot(results)
```

Saving models to file. When we knit our documents, all of the code in the document runs again. So far, not a big deal, but as we work with larger models and datasets this can become annoying and time-consuming. Advice for efficiently working with large models:  
1. Run the model (make sure it works like you want it to)  
2. Save the model to an R RDS file (a file type used to hold R objects). You would run the chunk below to do this.  
```{r}
saveRDS(rf_fit3, "rf_fit3.rds")
rm(rf_fit3)
```
3. Comment out your model code. For rf_fit3 these are in lines 173-188. You don't want those to run again when you knit.
4. Comment out your saveRDS code (don't want this to run again either)  
5. Below the commented out model code (lines 173-188), put a chunk that loads the saved RDS file  
```{R}
rf_fit3 = readRDS("rf_fit3.rds")
```
Ensemble Lecture

```{r}
library(titanic)
library(tidyverse)
library(caret)
library(rpart)
library(caretEnsemble) #new package
library(ranger)
library(VIM)
library(mice)
```

Titanic data read-in and preparation.  
```{r}
titanic = titanic::titanic_train

titanic = titanic %>% mutate(Survived = as.factor(Survived)) %>% 
  mutate(Survived = fct_recode(Survived, "No" = "0", "Yes" = "1" )) %>%
  mutate(Pclass = as.factor(Pclass)) %>% mutate(Sex = as.factor(Sex)) %>%
  mutate(Embarked = as.factor(Embarked)) %>% 
  mutate(Embarked = fct_recode(Embarked,"Unknown"="","Cherbourg"="C","Southampton"="S","Queenstown"="Q"))

titanic = titanic %>% select(c(-Cabin,-Embarked)) #getting rid of Embarked too since we know it's no good 

titanic = titanic %>% select(c("Survived","Pclass","Sex","Age","SibSp","Parch"))

imp_age = mice(titanic, m=1, method='pmm', printFlag=FALSE)

titanic_complete = complete(imp_age) 
summary(titanic_complete)
```

Splitting (in the manner that we have done many times)  
```{r}
set.seed(157)
split = createDataPartition(y=titanic_complete$Survived, p = .7, list = FALSE)
train = slice(titanic_complete,split)
test = slice(titanic_complete,-split)
```

In the next chunk we create trainControl object for caret. For ensemble models using caret we will seek to maximize the Area Under the Curve (AUC) of the ROC curve. Recall that higher values of AUC are indicative of better models. In order to build models in caret that focus on AUC, we need to modify the trainControl object. The modifications are indicated with comments below.    
```{r}
control = trainControl(
  method = "cv",
  number = 5, #to save time, we'll use 5 fold cross-validation rather than 10
  savePredictions = "final",
  classProbs = TRUE, #instructs caret to calculate probabilities (rather than providing final classifications)
  summaryFunction = twoClassSummary,  #enables calculation of AUC (must be present for AUC, should not necessary for accuracy)
  index=createResample(train$Survived) #new line needed (manages sampling in folds)
  )
```

Specify list of models to include in the ensemble. This step also builds the models in the list. For now,we'll build without parameter tuning.  
```{r}
set.seed(109)
model_list = caretList(
  x=train[,-1], y=train$Survived, #use all variables (except Survived) as predictors
  metric = "ROC", #specify that maximizing AUC is our objective
  trControl= control, #using the previously defined trControl object
  methodList=c("glm","ranger","rpart") #specifying the model methods to use
  #A note about logistic regression in caret: Caret does not do any stepwise removal or addition of variables!
  
  #Ignore the warning message after you run this, it's not a problem!
  )
```
The warning message(s) displayed after the models are built can be ignored.

Looking at the first six predictions. These are the predicted survival probabilities of the first six passengers in the training set (the first six rows) for the two models that we are using in this ensemble. I am just doing this a quick sanity check.  
```{r}
as.data.frame(predict(model_list, newdata=head(train)))
```

Ideally, our models should each exhibit "good" performance, but be uncorrelated with each other. We can check model correlation with the following code.  
```{r}
modelCor(resamples(model_list)) #show model correlation
```

Unfortunately, these models are at least somewhat correlated. However, we'll move ahead and construct the ensemble model anyway.
```{r}
ensemble = caretEnsemble(
  model_list, 
  metric="ROC",
  trControl=control #we already defined the trControl object
    )
```

Examine the ensemble.  
```{r}
summary(ensemble)
```
From the summary, we see that the resulting AUC (shown as ROC) for the ensemble is 0.87. 

We can then evaluate the performance of the ensemble on the training and testing sets.  
```{r}
#training set
pred_ensemble = predict(ensemble, train, type = "raw")
confusionMatrix(pred_ensemble,train$Survived)

#testing set
pred_ensemble_test = predict(ensemble, test, type = "raw")
confusionMatrix(pred_ensemble_test,test$Survived)
```

Let's repeat, but with parameter tuning. Running this might take a moment :) 
```{r}
ranger_grid = expand.grid(mtry = 1:5, #only going up to 5 since we only have 5 predictors (got rid of Embarked)
                          splitrule = c("gini","extratrees","hellinger"),
                          min.node.size = 1:5)

set.seed(109)
model_list2 = caretList(
  x=train[,-1], y=train$Survived, #use all variables (except Survived) as predictors
  metric = "ROC", #specify that maximizing AUC is our objective
  trControl= control, #using the previously defined trControl object
  methodList=c("glm","rpart"), #specifying the model methods to use that we WILL NOT TUNE (logistic regression and rpart only)
  tuneList=list( #specifies model(s) that WE WILL TUNE (ranger)
    ranger1=caretModelSpec(method="ranger",tuneGrid = ranger_grid)
  )
  )
  #A note about logistic regression in caret: Caret does not do any stepwise removal or addition of variables!
```

```{r}
modelCor(resamples(model_list2)) #show model correlation
```

```{r}
ensemble2 = caretEnsemble(
  model_list2, 
  metric="ROC",
  trControl=control #we already defined the trControl object
    )
```

Examine the ensemble.  
```{r}
summary(ensemble2)
```

```{r}
#training set
pred_ensemble2 = predict(ensemble2, train, type = "raw")
confusionMatrix(pred_ensemble2,train$Survived)

#testing set
pred_ensemble_test2 = predict(ensemble2, test, type = "raw")
confusionMatrix(pred_ensemble_test2,test$Survived)
```

#### Stacking
Now we will look at using stacking.  
```{r}
stack = caretStack(
  model_list2, #use the list of models already specified
  method ="glm", #stack models linearly
  metric ="ROC", #maximize AUC
  trControl = control #use existing train control object
  )
print(stack)
summary(stack)
```

Now use the stacked model to make predictions on the training and testing set.  
```{r}
#training set
pred_stack = predict(stack, train, type = "raw")
confusionMatrix(pred_stack,train$Survived)

#testing set
pred_stack_test = predict(stack, test, type = "raw")
confusionMatrix(pred_stack_test,test$Survived)
```
Not much different than with the non-stacked ensemble.

#### Credit Data Ensemble and Stacking  
Let's repeat this, but with the credit data.  
```{r}
credit = read_csv("CSData.csv")
```

Data cleaning and preparation (as done before)  
```{r}
credit = credit %>% mutate(SeriousDlqin2yrs = as.factor(SeriousDlqin2yrs)) %>% 
  mutate(SeriousDlqin2yrs = fct_recode(SeriousDlqin2yrs, "No" = "0", "Yes" = "1" )) 
credit = credit %>% filter(RevolvingUtilizationOfUnsecuredLines < 2)
credit = credit %>% filter(DebtRatio < 5)
credit = credit %>% filter(MonthlyIncome < 20000) %>% drop_na()
credit = credit %>% filter(NumberOfOpenCreditLinesAndLoans < 40)
credit = credit %>% filter(NumberOfTimes90DaysLate < 10)
credit = credit %>% filter(NumberRealEstateLoansOrLines < 10)
credit = credit %>% filter(NumberOfDependents < 10)
```

Now we'll split the data. Note that I am calling the training and testing sets, *train2 and test2*, respectively so as to not overwrite the sets from the titanic dataset.    
```{r}
set.seed(123) 
train.rows = createDataPartition(y = credit$SeriousDlqin2yrs, p=0.7, list = FALSE) #70% in training
train2 = slice(credit,train.rows,)
test2 = slice(credit,-train.rows,)
```

To build our ensemble, we are able to re-use much of the code from above. The control object has a different response in the index so I'm naming this object "control2".  
```{r}
control2 = trainControl(
  method = "cv",
  number = 3, #to save time, we'll use 3 fold cross-validation rather than 10
  savePredictions = "final",
  classProbs = TRUE, #instructs caret to calculate probabilities (rather than providing final classifications)
  summaryFunction = twoClassSummary, #enables calculation of AUC
  index=createResample(train2$SeriousDlqin2yrs), #new line needed (manages sampling in folds). Changed response variable to the correct dataset
  verboseIter = TRUE
  )
```

Specify list of models to include in the ensemble.  

We must be sure to change the model to reflect the credit dataset (we use the matrix notation) and the direct reference to the train2 dataset (not needed with matrix notation). 

Note that this next block of code *will* take some time to run.
WARNING: The next block of code required 3.8 hours to run
```{r}
start_time = Sys.time() #Put here to measure how long this code takes to run

ranger_grid = expand.grid(mtry = 1:8, #going to 8 
                          splitrule = c("gini","extratrees","hellinger"),
                          min.node.size = 1:3)

set.seed(109)
model_list3 = caretList(
  x=as.data.frame(train2[,-1]), y=train2$SeriousDlqin2yrs, 
  
  ##NOTE about the line above
  metric = "ROC", #specify that maximizing AUC is our objective
  trControl= control2, #using the previously defined trControl object
  methodList=c("glm","rpart"), #specifying the model methods to use
  tuneList=list( #specifies model(s) that WE WILL TUNE (ranger)
    ranger1=caretModelSpec(method="ranger",tuneGrid = ranger_grid)
  )
)

end_time = Sys.time()
end_time - start_time
```

```{r}
saveRDS(model_list3,"model_list3.rds")
```

```{r}
model_list3 = readRDS("model_list3.rds")
```

```{r}
modelCor(resamples(model_list3))
```
The ranger and glm models are pretty strongly correlated. Weaker correlation between other models.  

Building the ensemble.  
```{r}
ensemble3 = caretEnsemble(
  model_list3, 
  metric="ROC",
  trControl=control2)
```

Examine the ensemble.  
```{r}
summary(ensemble3)
```

```{r}
#training set
pred_ensemble3 = predict(ensemble3, train2, type = "raw")
confusionMatrix(pred_ensemble3,train2$SeriousDlqin2yrs)

#testing set
pred_ensemble_test3 = predict(ensemble3, test2, type = "raw")
confusionMatrix(pred_ensemble_test3,test2$SeriousDlqin2yrs)
```

On to stacking.  
```{r}
start_time = Sys.time() #Put here to measure how long this code takes to run

stack2 = caretStack(
  model_list3, #use the list of models already specified
  method ="glm", #stack models linearly
  metric ="ROC", #maximize AUC
  ###DO NOT use same trControl object here as you used to construct models
  trControl=trainControl(
    method="cv",
    number=10,
    savePredictions="final",
    classProbs=TRUE,
    summaryFunction=twoClassSummary
  )
)
end_time = Sys.time()
end_time - start_time
```

```{r}
print(stack2)
summary(stack2)
```

Now use the stacked model to make predictions on the training and testing set.  
```{r}
#training set
pred_stack2 = predict(stack2, train2, type = "raw")
confusionMatrix(pred_stack2,train2$SeriousDlqin2yrs)

#testing set
pred_stack_test2 = predict(stack2, test2, type = "raw")
confusionMatrix(pred_stack_test2,test2$SeriousDlqin2yrs)
```

## Neural Networks 

```{r}
options(tidyverse.quiet = TRUE)
library(titanic)
library(tidyverse)
library(mice) #package for imputation
library(VIM) #visualizing missingness
library(nnet) #for neural networks
library(caret)
```

Read in dataset   
```{r}
titanic = titanic::titanic_train
```

Structure and summary
```{r}
str(titanic)
summary(titanic)
```

Factor conversion and recoding  
```{r}
titanic = titanic %>% mutate(Survived = as.factor(Survived)) %>% 
  mutate(Survived = fct_recode(Survived, "No" = "0", "Yes" = "1" )) %>%
  mutate(Pclass = as.factor(Pclass)) %>% mutate(Sex = as.factor(Sex)) %>%
  mutate(Embarked = as.factor(Embarked)) %>% 
  mutate(Embarked = fct_recode(Embarked,"Unknown"="","Cherbourg"="C","Southampton"="S","Queenstown"="Q"))

#select only variables relevant to our analysis
titanic = titanic %>% select(c("Survived","Pclass","Sex","Age","SibSp","Parch"))

#impute
imp_age = mice(titanic, m=1, method='pmm', printFlag=FALSE)
summary(imp_age)

#Merge the imputed values into our titanic data frame  
titanic_complete = complete(imp_age) 
summary(titanic_complete)
```

Customary splitting  
```{r}
set.seed(123)
train.rows = createDataPartition(titanic_complete$Survived,p=0.7,list=FALSE)
train = slice(titanic_complete,train.rows)
test = slice(titanic_complete,-train.rows)
```

Next we build a neural network with no parameter tuning
```{r}
start_time = Sys.time() #for timing
fitControl = trainControl(method = "cv", 
                           number = 10)

set.seed(1234)
nnetBasic = train(x=titanic_complete[,-1], y= titanic_complete$Survived,
                 method = "nnet",
                 #tuneGrid = nnetGrid,
                 trControl = fitControl,
                 trace = FALSE)

end_time = Sys.time()
end_time-start_time
```

```{r}
nnetBasic
```

Predictions on the training set
```{r}
predNetBasic = predict(nnetBasic, train)
```

Confusion matrix
```{r}
confusionMatrix(predNetBasic, train$Survived, positive = "Yes")
```

Neural network with tuning
```{r}
start_time = Sys.time() #for timing
fitControl = trainControl(method = "cv", 
                           number = 10)

nnetGrid =  expand.grid(size = 1:8, #rule of thumb --> between # of input and # of output layers
                        decay = c(0.5, 0.1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7))
set.seed(1234)
nnetFit = train(x=titanic_complete[,-1],y=titanic_complete$Survived, 
                 method = "nnet",
                 trControl = fitControl,
                 tuneGrid = nnetGrid,
                 trace = FALSE)

end_time = Sys.time()
end_time-start_time
```

```{r}
nnetFit
```

Plot  
```{r}
plot(nnetFit)
```

Predictions  
```{r}
predNet = predict(nnetFit, train)
```

Confusion matrix
```{r}
confusionMatrix(predNet, train$Survived, positive = "Yes")
```

Testing set predictions
```{r}
predNet = predict(nnetFit, newdata = test)
```

Confusion matrix
```{r}
confusionMatrix(predNet, test$Survived, positive = "Yes")
```

#### Credit Data Neural Networks
Let's repeat this, but with the credit data.  
```{r}
credit = read_csv("CSData.csv")
```

Data cleaning and preparation (as done before)  
```{r}
credit = credit %>% mutate(SeriousDlqin2yrs = as.factor(SeriousDlqin2yrs)) %>% 
  mutate(SeriousDlqin2yrs = fct_recode(SeriousDlqin2yrs, "No" = "0", "Yes" = "1" )) 
credit = credit %>% filter(RevolvingUtilizationOfUnsecuredLines < 2)
credit = credit %>% filter(DebtRatio < 5)
credit = credit %>% filter(MonthlyIncome < 20000) %>% drop_na()
credit = credit %>% filter(NumberOfOpenCreditLinesAndLoans < 40)
credit = credit %>% filter(NumberOfTimes90DaysLate < 10)
credit = credit %>% filter(NumberRealEstateLoansOrLines < 10)
credit = credit %>% filter(NumberOfDependents < 10)
```

Now we'll split the data. Note that I am calling the training and testing sets, *train2 and test2*, respectively so as to not overwrite the sets from the titanic dataset.    
```{r}
set.seed(123) 
train.rows = createDataPartition(y = credit$SeriousDlqin2yrs, p=0.7, list = FALSE) #70% in training
train2 = slice(credit,train.rows,)
test2 = slice(credit,-train.rows,)
```

Neural network with tuning.
WARNING: This next chunk took about 38 minutes to run.  
```{r}
start_time = Sys.time() #for timing
fitControl = trainControl(method = "cv", 
                           number = 10)

nnetGrid =  expand.grid(size = seq(from = 1, to = 8, by = 1), #rule of thumb --> between # of input and # of output layers (input can be considered number of predictors, output is usually 1 for classification)
                        decay = c(0.5, 0.1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7))
set.seed(1234)
nnetFit2 = train(x=as.data.frame(train2[,-1]),y=train2$SeriousDlqin2yrs, 
                 #using as.data.frame to avoid tibble warnings
                 method = "nnet",
                 trControl = fitControl,
                 tuneGrid = nnetGrid,
                 trace = FALSE)

end_time = Sys.time()
end_time-start_time
```

```{r}
saveRDS(nnetFit2,"nnetfit2.rds")
rm(nnetFit2)
```

```{r}
nnetFit2 = readRDS("nnetfit2.rds")
```


```{r}
nnetFit2
plot(nnetFit2)
```

Predictions  
```{r}
predNet2 = predict(nnetFit2, train2)
```

Confusion matrix
```{r}
confusionMatrix(predNet2, train2$SeriousDlqin2yrs, positive = "Yes")
```

So, after all that, our model is equivalent to the naive (sigh). 

### Multiple Models (incl xgboost) Titanic Data

```{r}
#install.packages(c("titanic","tidyverse","caret","mice","VIM","MASS","ranger","randomForest","RColorBrewer","rpart","rattle","e1071","xgboost"))
library(titanic)
library(tidyverse)
library(caret)
library(mice)
library(VIM)
library(MASS)
library(ranger)
library(randomForest)
library(RColorBrewer)
library(rpart)
library(rattle)
library(e1071)
library(xgboost)
```

Load Titanic Data from the titanic package. 
```{r}
titanic = titanic::titanic_train
```

Factor conversion. Several of our variables are categorical and should be converted to factors.  
```{r}
titanic = titanic %>% mutate(Survived = as.factor(Survived)) %>% 
  mutate(Survived = fct_recode(Survived, "No" = "0", "Yes" = "1" )) %>%
  mutate(Pclass = as.factor(Pclass)) %>% mutate(Sex = as.factor(Sex)) %>%
  mutate(Embarked = as.factor(Embarked))
titanic = titanic %>% mutate(Embarked = fct_recode(Embarked,Unknown = ""))
titanic = titanic %>% dplyr::select(c("Survived","Pclass","Sex","Age","SibSp","Parch"))
imp_age = mice(titanic, m=1, method='pmm', printFlag=FALSE) #imputes age
titanic_complete = complete(imp_age) 
summary(titanic_complete)
```

Training/testing split
```{r}
set.seed(123) 
train.rows = createDataPartition(y = titanic_complete$Survived, p=0.7, list = FALSE) #70% in training
train = dplyr::slice(titanic_complete,train.rows) #NOTE!!! conflict with slice function
test = dplyr::slice(titanic_complete,-train.rows)
```

###xgboost model
Cannot have categorical data in our dataset for xgboost. Must convert the categorical variables to dummy variables. This uses a process called one-hot encoding. Normally R does this on its own (internally). For xgboost we HAVE to do this explicitly and manually!!    
```{r}
train_dummy = dummyVars(" ~ .", data = train) #creates dummy labels
train_xgb = data.frame(predict(train_dummy, newdata = train)) #converts variables in dataset to dummies
str(train_xgb)
```

```{r}
test_dummy = dummyVars(" ~ .", data = test) #creates dummy labels
test_xgb = data.frame(predict(test_dummy, newdata = test)) #converts variables in dataset to dummies
```

We only need one variable for Survived. Keep the Survived.Yes, remove Survived.No.  
```{r}
train_xgb = train_xgb %>% dplyr::select(-Survived.No) #NOTE! select conflict
test_xgb = test_xgb %>% dplyr::select(-Survived.No)
```

```{R}
str(train_xgb)
str(test_xgb)
```

We write the xgboost model in our (old) formula manner (y ~ x).  
```{r}
start_time = Sys.time() #for timing

set.seed(999)
ctrl = trainControl(method = "cv",
                     number = 10) #10 fold, k-fold cross-validation

fitxgb = train(as.factor(Survived.Yes)~.,
                data = train_xgb,
                method="xgbTree",
                trControl=ctrl)

end_time = Sys.time()
end_time-start_time
```

Saving, removing, and loading code (good habit for more complex models).  
```{r}
saveRDS(fitxgb,"fitxgb.rds")
rm(fitxgb) #removes fitxgb from the environment
```

```{r}
fitxgb = readRDS("fitxgb.rds")
```

```{R}
fitxgb
```

```{r}
predxgbtrain = predict(fitxgb, train_xgb)
#Pay attention to the syntax below since our variables are not factors  
confusionMatrix(as.factor(train_xgb$Survived.Yes), predxgbtrain,positive="1")
```

```{r}
predxgbtest = predict(fitxgb, test_xgb)
confusionMatrix(as.factor(test_xgb$Survived.Yes), predxgbtest,positive="1")
```

Next up is an xgb model with considerable tuning.  
```{r}
start_time = Sys.time() #for timing

set.seed(999)
ctrl = trainControl(method = "cv",
                     number = 10) #10 fold, k-fold cross-validation

tgrid = expand.grid(
  nrounds = 100, #50, 100, and 150 in default tuning
  max_depth = c(1,2,3,4), #1, 2, and 3 in default tuning
  eta = c(0.01, 0.1, 0.2, 0.3), #0.3 and 0.4 in default tuning
  gamma = 0, #fixed at 0 in default tuning
  colsample_bytree = c(0.6, 0.8, 1), #0.6 and 0.6 in default tuning
  min_child_weight = 1, #fixed at 1 in default tuning
  subsample = c(0.8, 1) #0.5, 0.75, and 1 in default tuning, we don't have much data so can choose a larger value
)

fitxgb2 = train(as.factor(Survived.Yes)~.,
                data = train_xgb,
                method="xgbTree",
                tuneGrid = tgrid,
                trControl=ctrl)

end_time = Sys.time()
end_time-start_time
```

```{r}
saveRDS(fitxgb2,"fitxgb2.rds")
rm(fitxgb2)
```

```{r}
fitxgb2 = readRDS("fitxgb2.rds")
```

```{R}
fitxgb2
plot(fitxgb2)
```

```{r}
predxgbtrain2 = predict(fitxgb2, train_xgb)
confusionMatrix(as.factor(train_xgb$Survived.Yes), predxgbtrain2,positive="1")
```

```{r}
predxgbtest2 = predict(fitxgb2, test_xgb)
confusionMatrix(as.factor(test_xgb$Survived.Yes), predxgbtest2,positive="1")
```